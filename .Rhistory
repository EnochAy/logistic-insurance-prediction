labels = make.names(levels(trainingOutcomes))))
train
View(train)
View(train)
model2 <- caretList(trainingOutcomes ~ ., data = train, methodList = c("glm", "nb"),
metric = "Kappa", tuneList = NULL, continue_on_fail = FALSE,
preprocess = c("center", "scale"), trControl = my_ctrl)
make.names(trainingOutcomes) ~ .,
make.names(trainingOutcomes ~ ., data = train, methodList = c("glm", "nb"),
metric = "Kappa", tuneList = NULL, continue_on_fail = FALSE,
preprocess = c("center", "scale"), trControl = my_ctrl)
train(Claim~., data = full_df, method="lda", metric=metric, preProc=c("center", "scale"), trControl=control)
library(mlbench)
library(caret)
library(doMC)
registerDoMC(cores=4)
# set-up test options
control <- trainControl(method="repeatedcv", number=10, repeats=3)
seed <- 7
metric <- "Accuracy"
#  train algorithms
# Linear Discriminant Analysis
set.seed(seed)
fit.lda <- train(Claim~., data = full_df, method="lda", metric=metric, preProc=c("center", "scale"), trControl=control)
train(Claim~., data = train, method="lda", metric=metric, preProc=c("center", "scale"), trControl=control)
train(Claim~., data = full, method="lda", metric=metric, preProc=c("center", "scale"), trControl=control)
train(trainingOutcomes~., data = train, method="lda", metric=metric, preProc=c("center", "scale"), trControl=control)
# Load the relevant libraries - do this every time
library(data.table)
library(corrplot)
library(ggplot2)
library (gcookbook)
library(caret)
library(hexbin)
library(leaps)
library(plyr)
library(plotly)
library(waffle)
library(dummies)
library(caTools)
library(wesanderson)
library(visreg)
library(car)
library(rpart)
library(leaps)
library(MASS)
library(skimr)
library(corrplot)
#Load train_data and test_data
library(readr)
train_data <- read_csv("Machine Learning Projects/Zindi(DSN Dataset)/train_data.csv")
test_data <- read_csv("Machine Learning Projects/Zindi(DSN Dataset)/test_data.csv")
#Join by rows == bind_rows(Test, train)
full <- bind_rows(train_data, test_data)
library(dplyr)
#Join by rows == bind_rows(Test, train)
full <- bind_rows(train_data, test_data)
dim(train_data)
dim(test_data)
dim(full)
library(dplyr)
library(caret)
library(caretEnsemble)
library(mice)
library(doParallel)
library(car)
train_data <- read_csv("Machine Learning Projects/Zindi(DSN Dataset)/train_data.csv", skip = 20, na.strings=”na")
test_data <- read_csv("Machine Learning Projects/Zindi(DSN Dataset)/test_data.csv", skip = 20, na.strings=”na")
library(dplyr)
full <- full_join(test_data, train_data) #Joined by columns Or
full <- bind_rows(train_data, test_data) #Join by rows == bind_rows(Test, train)
dim(train_data) #7160   14
dim(test_data) #3069   13
dim(full) #10229    14
train_data <- read_csv("Machine Learning Projects/Zindi(DSN Dataset)/train_data.csv", skip = 20, na.strings=na)
test_data <- read_csv("Machine Learning Projects/Zindi(DSN Dataset)/test_data.csv", skip = 20, na.strings=na)
library(dplyr)
full <- full_join(test_data, train_data) #Joined by columns Or
full <- bind_rows(train_data, test_data) #Join by rows == bind_rows(Test, train)
dim(train_data) #7160   14
dim(test_data) #3069   13
dim(full) #10229    14
train_data <- read_csv("Machine Learning Projects/Zindi(DSN Dataset)/train_data.csv", na.strings=NA)
test_data <- read_csv("Machine Learning Projects/Zindi(DSN Dataset)/test_data.csv", na.strings=NA)
train_data <- read.csv("Machine Learning Projects/Zindi(DSN Dataset)/train_data.csv", na.strings=NA)
test_data <- read.csv("Machine Learning Projects/Zindi(DSN Dataset)/test_data.csv", na.strings=NA)
l
train_data <- read.csv("Machine Learning Projects/Zindi(DSN Dataset)/train_data.csv", na.strings=NA)
test_data <- read.csv("Machine Learning Projects/Zindi(DSN Dataset)/test_data.csv", na.strings=NA)
dim(train_data) #7160   14
dim(test_data) #3069   13
dim(full) #10229    14
glimpse(train_data)
glimpse(test_data)
glimpse(full)
summary(train_data$Claim)
summary(test_data$Claim)
options(digits = 2)
prop.table(table(train_data$Claim))
prop.table(table(test_data$Claim))
options(scipen = 999)
summary_df <- do.call(cbind, lapply(train_data[,2:ncol(train_data)], summary))
summary_df_t <- as.data.frame(round(t(summary_df),0))
names(summary_df_t)[7] <- paste("Missing_values")
summary_df_t_2 <- summary_df_t %>%
mutate(obs = nrow(training_data),
Missing_prop = Missing_values / obs)
print(summary_df_t_2)
options(scipen = 999)
summary_df <- do.call(cbind, lapply(train_data[,2:ncol(train_data)], summary))
summary_df_t <- as.data.frame(round(t(summary_df),0))
names(summary_df_t)[7] <- paste("Missing_values")
summary_df_t_2 <- summary_df_t %>%
mutate(obs = nrow(train_data),
Missing_prop = Missing_values / obs)
print(summary_df_t_2)
View(summary_df)
View(summary_df_t)
View(summary_df_t_2)
print(summary_df_t_2)
summary_df_t_2 %>% summarise(Min = mean(Min.),
first_Q = mean(`1st Qu.`),
Median = median(Median),
Mean = mean(Mean),
third_Q = mean(`3rd Qu.`),
Max = max(Max.),
mean_MV = mean(Missing_values),
obs = mean(obs),
mean_MV_perc = mean_MV / obs)
rint(summary_df_t_2)
print(summary_df_t_2)
View(test_data)
View(train_data)
View(full)
View(summary_df_t_2)
View(summary_df_t)
View(summary_df)
do.call(bind, lapply(train_data[,2:ncol(train_data)], summary))
do.call(cbind, lapply(train_data[,2:ncol(train_data)], summary))
summary
summary_df
do.call(cbind, lapply(train_data[,
2:ncol(train_data)], summary))
summary_df <- do.call(cbind, lapply(train_data[,
2:ncol(train_data)], summary))
paste("Missing_values")
names(summary_df_t)[7]
summary_df_t_2 <- summary_df_t %>%
mutate(obs = nrow(train_data),
Missing_prop = Missing_values / obs)
summary_df_t_2
summary_df_t_2 %>% summarise(Min = mean(6088),
first_Q = mean(`3306`),
Median = median(6083),
Mean = mean(V4),
third_Q = mean(`13206`),
Max = max(31555),
mean_MV = mean(Missing_values),
obs = mean(obs),
mean_MV_perc = mean_MV / obs)
#replicate our sets
#create a new column "set" to label the observations
train_data$set <- "TRAIN"
test_data$set <- "TEST"
#merge them into 1 single set
full <- rbind(train_data, test_data)
dim(full)
full <- cbind(train_data, test_data)
dim(full)
full <- bind_rows(train_data, test_data)
dim(full)
full <- full_join(train_data, test_data)
dim(full) #10229    15
View(full)
#merge them into 1 single set
full <- bind_rows(train_data, test_data)
dim(full) #10229    15
View(full)
#merge them into 1 single set
full <- full_join(train_data, test_data)
dim(full) #10229    15
set.seed(123)
imputed_full <- mice(full_dataset,
m=1,
maxit = 5,
method = "mean",
seed = 500)
set.seed(123)
imputed_full <- mice(full, m=1, maxit = 5, method = "mean", seed = 500)
mice(full, m=1, maxit = 5, method = "mean", seed = 500)
\
?memory
?memory.size
memory.size()
memory.limit()
if(.Platform$OS.type == "windows") withAutoprint({
memory.size()
memory.size(TRUE)
memory.limit()
})
object.size()
object.size(full)
if(.Platform$OS.type == "windows") withAutoprint({
memory.size()
memory.size(TRUE)
memory.limit()
})
rm(list = ls(all.names = TRUE))
rm(list = ls())
load("C:/Users/Hp/Desktop/Data Science Projects/full_df.Rdata")
library(data.table)
library(corrplot)
library(ggplot2)
library (gcookbook)
library(caret)
library(hexbin)
library(leaps)
library(plyr)
library(plotly)
library(waffle)
library(dummies)
library(caTools)
library(wesanderson)
library(visreg)
library(car)
library(rpart)
library(leaps)
library(MASS)
library(skimr)
library(corrplot)
library(dplyr)
library(caret)
library(caretEnsemble)
library(mice)
library(doParallel)
library(car)
library(Hmisc)
library(mice)
str(full_dt)
str(df)
str(full_df)
library(h2o)
library(data.table)
library(corrplot)
library(ggplot2)
library (gcookbook)
library(caret)
library(hexbin)
library(leaps)
library(plyr)
library(plotly)
library(waffle)
library(dummies)
library(caTools)
library(wesanderson)
library(visreg)
library(car)
library(rpart)
library(leaps)
library(MASS)
library(skimr)
library(corrplot)
lapply(train_data["Claim"], factor,
labels = c("No Claim", "Claim"), levels = c(0,1))
lapply(full_df["Claim"], factor,
labels = c("No Claim", "Claim"), levels = c(0,1))
factor(full_df$Claim, levels = c(0,1), labels = c("No Claim", "Claim"))
factor(full_df$Claim, labels = c(0,1), levels = c("No Claim", "Claim"))
factor(full_df$Claim, levels = c(0,1), labels = c("No Claim", "Claim"))
full_df$Claim <- factor(full_df$Claim, levels = c(0,1), labels = c("No Claim", "Claim"))
View(full_df)
load("C:/Users/Hp/Desktop/Data Science Projects/full_df.Rdata")
View(full_df)
table(full_df$Claim)
round(prop.table(full_df$Claim)) * 100, digit = 1)
round(prop.table(table(full_df$Claim)) * 100, digit = 1)
round(prop.table(table(full_df$Claim)) * 100, digits = 1)
summary(full_df[c("radius_mean", "area_mean", "smoothness_mean")])
summary(full_df[c("Claim", "area_mean", "smoothness_mean")])
summary(full_df[c("Claim", "Insured_Period", "Building_Type")])
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
normalize(c(1,2,3,4,5))
normalize(c(10,20,30,40,50))
normalize(full_df)
full_numerics <- subset(full_df,
select = -c(Building_Painted, Building_Fenced, Settlement, Claim))#Subsetting numeric variables
corMatrix <-cor(full_numerics, use="complete.obs", method="pearson")#Check Correlations of numeric columns
round(corMatrix, 2)#round to two decimals
normalize(full_numerics)
summary(normalize(full_numerics))
full_df["Claim"]
full_df$Claim
factor(full_df$Claim, levels = c(0,1), labels = c("No Claim", "Claim"))
lapply(full_df["Claim"], factor,
labels = c("No Claim", "Claim"), levels = c(0,1))
#Or
full_df$Claim <- factor(full_df$Claim, levels = c(0,1), labels = c("No Claim", "Claim"))
full_cat <- subset(full_df, select = c(Building_Painted, Building_Fenced, Garden, Settlement))
full_df$key <- NULL #Deleting the variable key from the full_df
corrplot(cor(full_numerics), method = "circle")#correlation matricies package
corrplot(corMatrix, method="circle")
corrplot(corMatrix, method="square")
corrplot(corMatrix, method="number")
corrplot(corMatrix, method="shade")
corrplot(corMatrix, type = "upper")
corrplot.mixed(corMatrix)
#Categorical Variables with positive correlation with the target variable include:
#Building.Dimension, NumberOfWindows, Residential, Customer.Id, Insured_Period, Building_Type.
#Only YearOfObservation has negative correlation with the target variable
#
full_cat <- subset(full_df, select = c(Building_Painted, Building_Fenced, Claim, Settlement))
missing_vars <- function(x) {
var <- 0
missing <- 0
missing_prop <- 0
for (i in 1:length(names(x))) {
var[i] <- names(x)[i]
missing[i] <- sum(is.na(x[, i]))
missing_prop[i] <- missing[i] / nrow(x)
}
(missing_data <- data.frame(var = var, missing = missing, missing_prop = missing_prop) %>%
arrange(desc(missing_prop)))
}
missing_vars(test)
missing_vars(full_cat)
missing_vars(full_numerics)
set.seed(101)
library(caTools)
sample <- sample.split(full_df, SplitRatio = .70)
sample
train <- subset(full_df, sample == TRUE) #X_train
test  <- subset(full_df, sample == FALSE) #X_test
sample <- sample.split(full_df, SplitRatio = .80)
sample
train <- subset(full_df, sample == TRUE) #X_train
test  <- subset(full_df, sample == FALSE) #X_test
as.factor(train$Claim)
(train$Claim)
trainingOutcomes <- as.factor(train$Claim) #y_train
testOutcomes <- as.factor(test$Claim) #y_test
levels(testOutcomes)
glm(trainingOutcomes ~ ., data = train, family = binomial(link = "logit"))
mymodel <- glm(trainingOutcomes ~ ., data = train, family = binomial(link = "logit"))
summary(mymodel)
anova(mymodel)
effects(mymodel)
fitted.values(mymodel)
residuals(mymodel)
predict(mymodel, test, type = 'response')
glm(trainingOutcomes ~ ., data = train, family = binomial)
glm(trainingOutcomes ~ ., data = train, family = "binomial")
mymodel <- glm(trainingOutcomes ~ ., data = train, family = "binomial")
summary(mymodel)
mymodel <- glm(trainingOutcomes ~ ., family = "binomial",  data = train)
summary(mymodel)
pred_test <- predict(mymodel, newdata = test, type = 'response')
pred_test
summary(pred_test)
attach(test)
pred_test <- predict(mymodel, newdata = test, type = 'response')
summary(pred_test)
pred_test
pred <- predict(mymodel, newdata = test, type = 'response')
summary(pred)
rm(pred_test)
pred = ifelse(pred > .5, 1, 0)
table(pred, test$Claim)
pred
table(pred, test$Claim)
table(Actual_val = testOutcomes, predicted_value = pred_test > 0.5)
table(Actual_val = testOutcomes, predicted_value = pred > 0.5)
(confmatrix_test[[1,1]] + confmatrix_test[[2,2]]) / sum(confmatrix_test)
confmatrix_test <- table(Actual_val = testOutcomes, predicted_value = pred > 0.5)
confmatrix_test
#p
(confmatrix_test[[1,1]] + confmatrix_test[[2,2]]) / sum(confmatrix_test)
glm(trainingOutcomes ~ ., data = train, family = binomial(link = "logit"))
model.lr1 <- glm(trainingOutcomes ~ ., data = train, family = binomial(link = "logit"))
summary(model.lr1)
anova(model.lr1)
aic <- AIC(model.lr1) #[1] 5318.031
bic <- BIC(model.lr1) #[1] 5318.031
aic
bic
pred.model.lr1 <- predict(model.lr1, newdata = test) # validation predictions
pred.model.lr1
mean((testOutcomes - pred.model.lr1)^2) # mean prediction error == [1] 11.30005
load("C:/Users/Hp/Desktop/Data Science Projects/full_df.Rdata")
full_df$Claim
target_v <- full_df$Claim
trainingOutcomes.num <- train$Claim #numeric y_train
testOutcomes.num <- test$Claim #numeric y_test
as.numeric(train$Claim)
as.numeric(test$Claim) #numeric y_test
full_df$Claim
target_v[1:8037]
target_v[8038:10299] #numeric y_test
1262 +1000
count(target_v[8038:10299]) #numeric y_test
trainingOutcomes.num <- target_v[1:8037] #numeric y_train
count(trainingOutcomes.num)
target_v[0:8037]
7037+1000
target_v[8038:10299]
1262+1000
target_v <- full_df$Claim
trainingOutcomes.num <- target_v[0:8037] #numeric y_train
testOutcomes.num <- target_v[8038:10299] #numeric y_test
bic
meanPred <- mean((testOutcomes.num - pred.model.lr1)^2) # mean prediction error == [1] 11.30005
stdError <- sd((testOutcomes.num - pred.model.lr1)^2)/length(testOutcomes.num) # std error==0.008451522
mean((trainingOutcomes.num - pred.model.lr1)^2) # mean prediction error == [1] 11.30005
target_v[0:2191]
1191+1000
target_v[0:2192]
1192+1000
target_v[2193:10229]
7037+1000
trainingOutcomes.num <- target_v[2193:10229] #numeric y_train
testOutcomes.num <- target_v[0:2192] #numeric y_test
meanPred <- mean((testOutcomes.num - pred.model.lr1)^2) # mean prediction error == [1] 11.30005
stdError <- sd((testOutcomes.num - pred.model.lr1)^2)/length(testOutcomes.num) # std error==0.008451522
meanPred
stdError
modelMetrics <- data.frame( "Model" = character(), "adjRsq" = integer(),  "AIC"= integer(), "BIC"= integer(),
"Mean Prediction Error"= integer(), "Standard Error"= integer(), stringsAsFactors=FALSE)
# Append a row with metrics for model.lr1
modelMetrics[nrow(modelMetrics) + 1, ] <- c( "model.lr1", 0.279, aic, bic, meanPred, stdError)
modelMetrics
step <- stepAIC(model.lr1, direction="backward")
step$anova # display results
full_df$Claim <- factor(full_df$Claim, levels = c(0,1), labels = c("No Claim", "Claim"))
meanPred <- mean((testOutcomes.num - pred.model.lr1)^2) # mean prediction error == [1] 11.30005,
#705.756 new
stdError <- sd((testOutcomes.num - pred.model.lr1)^2)/length(testOutcomes.num)
# std error==0.008451522, [1] 2.905017e-09 new
### Create a list to store the model metrics
modelMetrics <- data.frame( "Model" = character(), "adjRsq" = integer(),  "AIC"= integer(), "BIC"= integer(),
"Mean Prediction Error"= integer(), "Standard Error"= integer(), stringsAsFactors=FALSE)
# Append a row with metrics for model.lr1
modelMetrics[nrow(modelMetrics) + 1, ] <- c( "model.lr1", 0.279, aic, bic, meanPred, stdError)
modelMetrics
meanPred
stdError
step <- stepAIC(model.lr1, direction="backward")
step$anova # display results
model.bkwd.lr1 <- glm(trainingOutcomes ~ Claim, train, family = binomial(link = "logit"))
#model.bkwd.lr1 <- glm(trainingOutcomes ~ Insured_Period + Residential + Building_Painted + Building_Fenced +
#                      Building.Dimension + Building_Type + Date_of_Occupancy +
#                     NumberOfWindows + Geo_Code, train, family = binomial(link = "logit"))
summary(model.bkwd.lr1)
aic <- AIC(model.bkwd.lr1) #5327.038
bic <- BIC(model.bkwd.lr1) #5394.95
#
pred.model.bkwd.lr1 <- predict(model.bkwd.lr1, newdata = test) # validation predictions
meanPred <- mean((testOutcomes.num - pred.model.bkwd.lr1 )^2) #  9.935557 mean prediction error
stdError <- sd((testOutcomes.num - pred.model.bkwd.lr1 )^2)/length(testOutcomes.num)#0.001285681std error
#
modelMetrics[nrow(modelMetrics) + 1, ] <- c( "model.bkwd.lr1", 0.2795, aic, bic, meanPred, stdError)
modelMetrics
#
corData <- train[ -c(5:8) ]
head(corData)
corMatrix <- cor(corData, use="complete.obs", method="pearson")
corrplot(corMatrix, type = "upper", order = "hclust", col = c("black", "white"),
bg = "lightblue", tl.col = "black")
#Create the model with some of the most highly correlated variables as displayed in the above graph.
model.lr2 <- glm(trainingOutcomes ~ Building.Dimension + Customer.Id + NumberOfWindows + Residential +
Insured_Period + Geo_Code, train, family = binomial(link = "logit"))
summary(model.lr2)
#Mea
aic <- AIC(model.lr2)
bic <- BIC(model.lr2)
pred.model.lr2 <- predict(model.lr2, newdata = test) # validation predictions
meanPred <- mean((testOutcomes.num - pred.model.lr2 )^2) # mean prediction error
stdError <- sd((testOutcomes.num - pred.model.lr2 )^2)/length(testOutcomes.num) # std error
# Append a row to our modelMetrics
modelMetrics[nrow(modelMetrics) + 1, ] <- c( "model.lr2", 0.2353, aic, bic, meanPred, stdError)
modelMetrics
model.bestSub = regsubsets(trainingOutcomes ~ ., train, nvmax =25)
summary(model.bestSub)
reg.summary = summary(model.bestSub)
which.min (reg.summary$bic )
which.max (reg.summary$adjr2 ) # just for fun
plot(reg.summary$bic ,xlab=" Number of Variables ",ylab=" BIC",type="l")
points(6, reg.summary$bic [6], col =" red",cex =2, pch =20)
coef(model.bestSub, 6)
bestSubModel <- glm(trainingOutcomes ~ Insured_Period + Residential + Building_Painted + Building_Fenced +
Building_Type + Geo_Code, data=train, family = binomial(link = "logit"))
summary(bestSubModel)
aic <- AIC(bestSubModel)
bic <- BIC(bestSubModel)
# mean Prediction Error and Std Error - lower is better
pred.bestSubModel <- predict(bestSubModel, newdata = test) # validation predictions
meanPred <- mean((testOutcomes.num - pred.bestSubModel)^2)# mean prediction error
stdError <- sd((testOutcomes.num - pred.bestSubModel)^2)/length(testOutcomes.num) # std error
modelMetrics[nrow(modelMetrics) + 1, ] <- c( "bestSubModel", 0.1789, aic, bic, meanPred, stdError)
modelMetrics
#
par(mfrow = c(2, 2))  # Split the plotting panel into a 2 x 2 grid
plot(model.bkwd.lr1)
cutoff <- 4/((nrow(train)-length(model.bkwd.lr1$coefficients)-2))#Create Cooks Distance Plot
plot(model.bkwd.lr1, which=4, cook.levels=cutoff)
#visualize the model
visreg2d(model.bkwd.lr1, "Insured_Period", "Residential", plot.type="persp" )
visreg2d(model.bkwd.lr1, "Insured_Period", "Residential", plot.type="image" )
visreg(model.bkwd.lr1, "Insured_Period")
Predictions <- predict(model.bkwd.lr1, test) # test predictions
plot(trainingOutcomes, type ='l', lty = 1.8, col = 'red')
lines(Predictions, type = 'l', lty = 1.8, col = 'green')
